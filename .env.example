# PDF Processing Pipeline - Environment Configuration
# Copy this file to .env and update with your actual values

# ============================================================
# AWS Configuration (Optional - for Textract)
# ============================================================
AWS_REGION=us-east-1
AWS_TEXTRACT_MAX_SIZE_MB=10

# AWS Credentials (if not using IAM roles)
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key

# ============================================================
# Processing Configuration
# ============================================================
DEBUG=false

# ============================================================
# Chunking Configuration
# ============================================================
# Chunking strategy: "langchain" (section-aware) or "langchain_simple" (size-based)
# Recommended: "langchain" for best similarity scores (see evaluation/CHUNKING_COMPARISON_RESULTS.md)
CHUNKER_TYPE=langchain

# Maximum chunk size in characters
MAX_CHUNK_SIZE=2000

# Overlap between chunks (helps maintain context)
CHUNK_OVERLAP=200

# ============================================================
# Embedding Configuration
# ============================================================
# Model to use for generating embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Embedding dimension (must match your model)
# - all-MiniLM-L6-v2: 384 (recommended, fast, good quality)
# - all-mpnet-base-v2: 768 (slower, higher quality)
# - paraphrase-multilingual-MiniLM-L12-v2: 384 (multilingual)
EMBEDDING_DIMENSION=384

# Batch size for embedding generation (larger = faster but more memory)
EMBEDDING_BATCH_SIZE=32

# Normalize embeddings (recommended for cosine similarity)
EMBEDDING_NORMALIZE=true

# AWS Bedrock (alternative to local embeddings)
# BEDROCK_MODEL_ID=amazon.titan-embed-text-v1

# ============================================================
# Database Configuration (PostgreSQL + pgvector)
# ============================================================
# Connection string format: postgresql://username:password@host:port/database
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/pdf_rag

# Database connection pool size
DB_POOL_SIZE=10

# Echo SQL queries (useful for debugging)
DB_ECHO=false

# ============================================================
# Search Configuration
# ============================================================
# Number of results to return by default
SEARCH_TOP_K=10

# Minimum similarity score threshold (0.0 to 1.0)
SIMILARITY_THRESHOLD=0.7

# ============================================================
# API Configuration
# ============================================================
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# ============================================================
# Storage Configuration
# ============================================================
UPLOAD_DIR=./data/uploads
TEMP_DIR=./data/temp

# ============================================================
# SETUP INSTRUCTIONS
# ============================================================
#
# 1. Install PostgreSQL:
#    - Ubuntu/Debian: sudo apt-get install postgresql postgresql-contrib
#    - macOS: brew install postgresql
#    - Windows: Download from https://www.postgresql.org/download/windows/
#
# 2. Install pgvector extension:
#    - Follow instructions at: https://github.com/pgvector/pgvector
#    - Ubuntu/Debian: sudo apt-get install postgresql-16-pgvector
#    - macOS: brew install pgvector
#
# 3. Create database:
#    createdb pdf_rag
#
# 4. Initialize database:
#    python scripts/init_database.py
#
# 5. (Optional) Create test database:
#    createdb pdf_rag_test
#    export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/pdf_rag_test"
#    python -m pytest tests/unit/test_vector_store.py
